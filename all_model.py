
import streamlit as st
import pandas as pd
import numpy as np
import statsmodels.api as sm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.linear_model import SGDRegressor, LogisticRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, classification_report, f1_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

st.title('ğŸ§® íšŒê·€ ë° ë¶„ë¥˜ ë¶„ì„ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜')

# ë°ì´í„° ì—…ë¡œë“œ
st.header('1. ë°ì´í„° ì—…ë¡œë“œ')
uploaded_file = st.file_uploader("CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["csv"])

if uploaded_file is not None:
    data = pd.read_csv(uploaded_file)
    st.subheader('ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°')
    st.dataframe(data.head())

    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    st.subheader('ê²°ì¸¡ì¹˜ ì²˜ë¦¬')
    if st.checkbox('ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°'):
        data.dropna(inplace=True)
        st.write('ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.')

    # ë³€ìˆ˜ ì„ íƒ
    st.header('2. ë³€ìˆ˜ ì„ íƒ')
    target = st.selectbox('ë°˜ì‘ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.', data.columns)
    # ëª¨ë“  ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ê³  ì œì™¸í•  ë³€ìˆ˜ ì„ íƒ ê¸°ëŠ¥ ì¶”ê°€
    all_features = st.checkbox('ëª¨ë“  ë³€ìˆ˜ë¥¼ ì„¤ëª…ë³€ìˆ˜ë¡œ ì„ íƒ')
    if all_features:
        exclude_features = st.multiselect('ì œì™¸í•  ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.', data.columns.drop(target))
        features = list(set(data.columns.drop(target)) - set(exclude_features))
    else:
        features = st.multiselect('ì„¤ëª…ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.', data.columns.drop(target))

    if features:
        # ë°ì´í„° ì‹œê°í™”
        st.header('3. ë°ì´í„° ì‹œê°í™”')
        if st.checkbox('ì‚°ì ë„ í–‰ë ¬ ë³´ê¸°'):
            if len(features) <= 5:
                sns.pairplot(data[[target] + features])
                st.pyplot(plt)
            else:
                st.write('ë³€ìˆ˜ ê°œìˆ˜ê°€ ë§ì•„ ì‚°ì ë„ í–‰ë ¬ì„ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')

        # ëª¨ë¸ ì„ íƒ ë° ì„¤ì •
        st.header('4. ëª¨ë¸ ì„ íƒ ë° ì„¤ì •')
        model_type = st.selectbox('ëª¨ë¸ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”.', ['íšŒê·€', 'ë¶„ë¥˜'])

        if model_type == 'íšŒê·€':
            regression_type = st.selectbox('íšŒê·€ ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.', ['ì„ í˜• íšŒê·€', 'Decision Tree íšŒê·€', 'Random Forest íšŒê·€'])
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
            if regression_type == 'ì„ í˜• íšŒê·€':
                loss_function = st.selectbox('ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.', ['ì¼ë°˜ ì„ í˜• íšŒê·€ (OLS)', 'Huber íšŒê·€'])
                if loss_function == 'Huber íšŒê·€':
                    epsilon = st.number_input('Huber epsilon ê°’', min_value=1.0, max_value=10.0, value=1.35)
            elif regression_type in ['Decision Tree íšŒê·€', 'Random Forest íšŒê·€']:
                auto_max_depth = st.checkbox('Early Stoppingì„ ì‚¬ìš©í•˜ì—¬ max_depth ê²°ì •')
                if auto_max_depth:
                    #if st.button('Early Stopping ì‹¤í–‰'):
                    if True:
                        # Early Stopping ì‹¤í–‰
                        test_size = 0.2
                        tolerance = 3
                        X_model, X_unused, y_model, y_unused = train_test_split(data[features + [target]], data[target], test_size=0.001, random_state=42)
                        X = X_model[features]
                        y = X_model[target]
                        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)
                        best_depth = None
                        if model_type == 'íšŒê·€':
                            best_score = np.inf
                        else:
                            best_score = 0
                        no_improvement_count = 0

                        for depth in range(1, 20):
                            if regression_type == 'Decision Tree íšŒê·€':
                                temp_model = DecisionTreeRegressor(max_depth=depth)
                            elif regression_type == 'Random Forest íšŒê·€':
                                temp_model = RandomForestRegressor(n_estimators=100, max_depth=depth, n_jobs=-1)

                            temp_model.fit(X_train, y_train)
                            y_pred = temp_model.predict(X_val)
                            score = mean_squared_error(y_val, y_pred)

                            if score < best_score:
                                best_score = score
                                best_depth = depth
                                no_improvement_count = 0
                            else:
                                no_improvement_count += 1

                            if no_improvement_count >= tolerance:
                                #st.write(f"ì¡°ê¸° ì¢…ë£Œ: ê°œì„  ì—†ì´ {tolerance}íšŒ ë°˜ë³µ, ìµœì ì˜ max_depth={best_depth}")
                                break
                        else:
                            st.write(f"ìµœì ì˜ max_depth: {best_depth}")

                        st.session_state['best_depth'] = best_depth
                    if 'best_depth' in st.session_state:
                        max_depth = st.number_input('íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ (max_depth)', min_value=1, max_value=20, value=st.session_state['best_depth'])
                    else:
                        st.warning('Early Stoppingì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.')
                else:
                    max_depth = st.number_input('íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ (max_depth)', min_value=1, max_value=20, value=5)
            else:
                max_depth = None
        else:
            classification_type = st.selectbox('ë¶„ë¥˜ ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.', ['ë¡œì§€ìŠ¤í‹± ë¶„ë¥˜', 'Decision Tree ë¶„ë¥˜', 'Random Forest ë¶„ë¥˜'])
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
            if classification_type in ['Decision Tree ë¶„ë¥˜', 'Random Forest ë¶„ë¥˜']:
                auto_max_depth = st.checkbox('Early Stoppingì„ ì‚¬ìš©í•˜ì—¬ max_depth ê²°ì •')
                if auto_max_depth:
                    #if st.button('Early Stopping ì‹¤í–‰'):
                    if True:
                        # Early Stopping ì‹¤í–‰
                        test_size = 0.2
                        tolerance = 3
                        X_model, X_unused, y_model, y_unused = train_test_split(data[features + [target]], data[target], test_size=0.001, random_state=42)
                        X = X_model[features]
                        y = X_model[target]
                        le = LabelEncoder()
                        y = le.fit_transform(y)
                        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=42)
                        best_depth = None
                        best_score = 0
                        no_improvement_count = 0

                        for depth in range(1, 20):
                            if classification_type == 'Decision Tree ë¶„ë¥˜':
                                temp_model = DecisionTreeClassifier(max_depth=depth)
                            elif classification_type == 'Random Forest ë¶„ë¥˜':
                                temp_model = RandomForestClassifier(n_estimators=100, max_depth=depth, n_jobs=-1)

                            temp_model.fit(X_train, y_train)
                            y_pred = temp_model.predict(X_val)
                            score = accuracy_score(y_val, y_pred)

                            if score > best_score:
                                best_score = score
                                best_depth = depth
                                no_improvement_count = 0
                            else:
                                no_improvement_count += 1

                            if no_improvement_count >= tolerance:
                                #st.write(f"ì¡°ê¸° ì¢…ë£Œ: ê°œì„  ì—†ì´ {tolerance}íšŒ ë°˜ë³µ, ìµœì ì˜ max_depth={best_depth}")
                                break
                        else:
                            st.write(f"ìµœì ì˜ max_depth: {best_depth}")

                        st.session_state['best_depth'] = best_depth
                    if 'best_depth' in st.session_state:
                        max_depth = st.number_input('íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ (max_depth)', min_value=1, max_value=20, value=st.session_state['best_depth'])
                    else:
                        st.warning('Early Stoppingì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.')
                else:
                    max_depth = st.number_input('íŠ¸ë¦¬ì˜ ìµœëŒ€ ê¹Šì´ (max_depth)', min_value=1, max_value=20, value=5)
            else:
                max_depth = None

        # ë°ì´í„° ë¶„ë¦¬
        X = data[features]
        y = data[target]

        # ë¶„ë¥˜ ëª¨ë¸ì˜ ê²½ìš° ë°˜ì‘ë³€ìˆ˜ ë¼ë²¨ ì¸ì½”ë”©
        if model_type == 'ë¶„ë¥˜' and classification_type != 'ë¡œì§€ìŠ¤í‹± ë¶„ë¥˜':
            le = LabelEncoder()
            y = le.fit_transform(y)

        # ëª¨ë¸ í•™ìŠµ ë° ê²°ê³¼ í™•ì¸
        st.header('5. ëª¨ë¸ í•™ìŠµ ë° ê²°ê³¼ í™•ì¸')
        if st.button('ëª¨ë¸ í•™ìŠµ ì‹œì‘'):
            if model_type == 'íšŒê·€':
                if regression_type == 'ì„ í˜• íšŒê·€':
                    X_const = sm.add_constant(X)
                    if loss_function == 'ì¼ë°˜ ì„ í˜• íšŒê·€ (OLS)':
                        model = sm.OLS(y, X_const).fit()
                    elif loss_function == 'Huber íšŒê·€':
                        model = sm.RLM(y, X_const, M=sm.robust.norms.HuberT(t=epsilon)).fit()

                    st.subheader('ëª¨ë¸ ê²°ê³¼')
                    st.write(model.summary())

                    predictions = model.predict(X_const)
                    residuals = model.resid

                    # RÂ² ì¶œë ¥
                    st.subheader('ëª¨ë¸ í‰ê°€ ì§€í‘œ')
                    st.write('MSE (Mean Squared Error):', mean_squared_error(y, predictions))
                    st.write('MAE (Mean Absolute Error):', mean_absolute_error(y, predictions))
                    st.write('RÂ² (ê²°ì •ê³„ìˆ˜):', r2_score(y, predictions))

                elif regression_type == 'Decision Tree íšŒê·€':
                    model = DecisionTreeRegressor(max_depth=int(max_depth))
                    model.fit(X, y)
                    predictions = model.predict(X)
                    residuals = y - predictions
                    st.subheader('ëª¨ë¸ ê²°ê³¼')
                    st.write('ë³€ìˆ˜ ì¤‘ìš”ë„:')
                    importance_df = pd.DataFrame({'Features': features, 'Variable Importance': model.feature_importances_})
                    st.write(importance_df)
                    # í‰ê°€ ì§€í‘œ
                    st.subheader('ëª¨ë¸ í‰ê°€ ì§€í‘œ')
                    st.write('MSE (Mean Squared Error):', mean_squared_error(y, predictions))
                    st.write('MAE (Mean Absolute Error):', mean_absolute_error(y, predictions))
                    # íŠ¸ë¦¬ ì‹œê°í™”
                    st.subheader('íŠ¸ë¦¬ ì‹œê°í™”')
                    fig, ax = plt.subplots(figsize=(12, 8))
                    plot_tree(model, feature_names=features, filled=True, ax=ax)
                    st.pyplot(fig)

                elif regression_type == 'Random Forest íšŒê·€':
                    model = RandomForestRegressor(n_estimators=100, max_depth=int(max_depth), n_jobs=-1)
                    model.fit(X, y)
                    predictions = model.predict(X)
                    residuals = y - predictions
                    st.subheader('ëª¨ë¸ ê²°ê³¼')
                    st.write('ë³€ìˆ˜ ì¤‘ìš”ë„:')
                    importance_df = pd.DataFrame({'Features': features, 'Variable Importance': model.feature_importances_})
                    st.write(importance_df)
                    # í‰ê°€ ì§€í‘œ
                    st.subheader('ëª¨ë¸ í‰ê°€ ì§€í‘œ')
                    st.write('MSE (Mean Squared Error):', mean_squared_error(y, predictions))
                    st.write('MAE (Mean Absolute Error):', mean_absolute_error(y, predictions))

            else:
                # ë¶„ë¥˜ ëª¨ë¸
                if classification_type == 'ë¡œì§€ìŠ¤í‹± ë¶„ë¥˜':
                    model = LogisticRegression(max_iter=1000)
                    model.fit(X, y)
                    predictions = model.predict(X)
                    st.subheader('ëª¨ë¸ ê²°ê³¼')
                    st.write('íšŒê·€ ê³„ìˆ˜:')
                    coef_df = pd.DataFrame({'Features': features, 'Coefficients': model.coef_[0]})
                    st.write(coef_df)
                    st.write('ì ˆí¸ (Intercept):', model.intercept_[0])
                    # í‰ê°€ ì§€í‘œ
                    st.subheader('ëª¨ë¸ í‰ê°€ ì§€í‘œ')
                    st.write('ì •í™•ë„ (Accuracy):', accuracy_score(y, predictions))
                    st.write('ë¶„ë¥˜ ë¦¬í¬íŠ¸:')
                    st.text(classification_report(y, predictions))
                else:
                    model = None
                    if classification_type == 'Decision Tree ë¶„ë¥˜':
                        model = DecisionTreeClassifier(max_depth=int(max_depth))
                    elif classification_type == 'Random Forest ë¶„ë¥˜':
                        model = RandomForestClassifier(n_estimators=100, max_depth=int(max_depth), n_jobs=-1)

                    model.fit(X, y)
                    predictions = model.predict(X)
                    st.subheader('ëª¨ë¸ ê²°ê³¼')
                    st.write('ë³€ìˆ˜ ì¤‘ìš”ë„:')
                    importance_df = pd.DataFrame({'Features': features, 'Variable Importance': model.feature_importances_})
                    st.write(importance_df)
                    # í‰ê°€ ì§€í‘œ
                    st.subheader('ëª¨ë¸ í‰ê°€ ì§€í‘œ')
                    st.write('ì •í™•ë„ (Accuracy):', accuracy_score(y, predictions))
                    st.write('ë¶„ë¥˜ ë¦¬í¬íŠ¸:')
                    st.text(classification_report(y, predictions))

                    # íŠ¸ë¦¬ ì‹œê°í™” (Decision Treeì˜ ê²½ìš°)
                    if classification_type == 'Decision Tree ë¶„ë¥˜':
                        st.subheader('íŠ¸ë¦¬ ì‹œê°í™”')
                        fig, ax = plt.subplots(figsize=(12, 8))
                        plot_tree(model, feature_names=features, class_names=le.classes_, filled=True, ax=ax)
                        st.pyplot(fig)

            # ë…ë¦½ë³€ìˆ˜ê°€ í•˜ë‚˜ì¸ ê²½ìš° ì˜ˆì¸¡ê°’ ì‹œê°í™”
            if model_type == 'íšŒê·€' and len(features) == 1:
                st.subheader('ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”')
                X_sorted = X.sort_values(by=features[0])
                X_plot = np.linspace(X[features[0]].min(), X[features[0]].max(), 100)
                X_plot_df = pd.DataFrame({features[0]: X_plot})

                if regression_type == 'ì„ í˜• íšŒê·€':
                    X_plot_const = sm.add_constant(X_plot_df)
                    y_plot = model.predict(X_plot_const)
                else:
                    y_plot = model.predict(X_plot_df)

                fig, ax = plt.subplots()
                ax.scatter(X[features[0]], y, label='Actual')
                ax.plot(X_plot, y_plot, color='red', label='Predicted')
                ax.set_xlabel(features[0])
                ax.set_ylabel(target)
                ax.legend()
                st.pyplot(fig)

            # íšŒê·€ ëª¨ë¸ì¸ ê²½ìš°ì—ë§Œ ì”ì°¨ í”Œë¡¯ í‘œì‹œ
            if model_type == 'íšŒê·€':
                st.subheader('ì”ì°¨ í”Œë¡¯')
                fig, ax = plt.subplots()
                ax.scatter(predictions, residuals)
                ax.axhline(0, color='red', linestyle='--')
                ax.set_xlabel('Predicted Values')
                ax.set_ylabel('Residuals')
                st.pyplot(fig)

            # ê²°ê³¼ ì €ì¥
            st.header('6. ê²°ê³¼ ì €ì¥')
            if st.button('ëª¨ë¸ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ'):
                if model_type == 'íšŒê·€':
                    if regression_type == 'ì„ í˜• íšŒê·€':
                        result_df = model.summary2().tables[1]
                        result_df.to_csv('model_results.csv')
                    else:
                        results = importance_df
                        results.to_csv('model_results.csv', index=False)
                else:
                    results = importance_df if 'importance_df' in locals() else coef_df
                    results.to_csv('model_results.csv', index=False)
                st.write('ëª¨ë¸ ê²°ê³¼ë¥¼ model_results.csv íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.')

else:
    st.info('CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.')
